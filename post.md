# Patient Knowledge Distillation for BERT Model Compression
## Paper Review
* Abstract

 1. Introduction
 2. Related Work
 3. Patient Knowledge Distillation
 4. Experiments
 5. Conclusion 

* Post2
* Post3
